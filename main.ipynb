{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.    0.    1.  ]\n",
      " [ 1.08  1.68  2.38]\n",
      " [-0.83  1.82  2.49]\n",
      " [-1.97  0.28  2.15]\n",
      " [-1.31 -1.51  2.59]\n",
      " [ 0.57 -1.91  4.32]]\n"
     ]
    }
   ],
   "source": [
    "# Observations\n",
    "P = np.array([[+2.00, +0.00, +1.00],\n",
    "              [+1.08, +1.68, +2.38],\n",
    "              [-0.83, +1.82, +2.49],\n",
    "              [-1.97, +0.28, +2.15],\n",
    "              [-1.31, -1.51, +2.59],\n",
    "              [+0.57, -1.91, +4.32]], dtype=float)\n",
    "print(P)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Weights: \n",
      " [[ 1.46933333  2.126       0.79933333]\n",
      " [-0.44171429 -0.59028571  0.48257143]]\n",
      "Residual Error: \n",
      " 5.4603563513509055e-21\n"
     ]
    }
   ],
   "source": [
    "# Time\n",
    "T = np.array([[x ** n for n in range(2)] for x in [1, 2, 3, 4, 5, 6]]).reshape(len(P), -1)\n",
    "# print(X)\n",
    "\n",
    "\n",
    "# Model Weights\n",
    "W = np.ones((2, 3))\n",
    "\n",
    "\n",
    "# print(W)\n",
    "\n",
    "\n",
    "# def linear_gradient(objective_arguments):\n",
    "#     # This is the derivative of the objective function with respect to each weight.\n",
    "#     return np.array([[12 * objective_arguments[0, 0] +  42 * objective_arguments[1, 0] +   23 / 25],\n",
    "#                      [42 * objective_arguments[0, 0] + 182 * objective_arguments[1, 0] +  467 / 25],\n",
    "#                      [12 * objective_arguments[0, 1] +  42 * objective_arguments[1, 1] -   18 / 25],\n",
    "#                      [42 * objective_arguments[0, 1] + 182 * objective_arguments[1, 1] +  907 / 50],\n",
    "#                      [12 * objective_arguments[0, 2] +  42 * objective_arguments[1, 2] - 1493 / 50],\n",
    "#                      [42 * objective_arguments[0, 2] + 182 * objective_arguments[1, 2] -  607 /  5]], dtype=float).reshape((3, 2)).T\n",
    "\n",
    "\n",
    "# def quadratic_gradient(objective_arguments):\n",
    "#     # This is the derivative of the objective function with respect to each weight.\n",
    "#     return np.array([[ 12 * objective_arguments[0, 0] +  42 * objective_arguments[1, 0] +  182 * objective_arguments[2, 0] +   23 / 25],\n",
    "#                      [ 42 * objective_arguments[0, 0] + 182 * objective_arguments[1, 0] +  882 * objective_arguments[2, 0] +  467 / 25],\n",
    "#                      [182 * objective_arguments[0, 0] + 882 * objective_arguments[1, 0] + 4550 * objective_arguments[2, 0] +  449 /  5],\n",
    "#                      [ 12 * objective_arguments[0, 1] +  42 * objective_arguments[1, 1] +  182 * objective_arguments[2, 1] -   18 / 25],\n",
    "#                      [ 42 * objective_arguments[0, 1] + 182 * objective_arguments[1, 1] +  882 * objective_arguments[2, 1] +  907 / 50],\n",
    "#                      [182 * objective_arguments[0, 1] + 882 * objective_arguments[1, 1] + 4550 * objective_arguments[2, 1] + 7893 / 50],\n",
    "#                      [ 12 * objective_arguments[0, 2] +  42 * objective_arguments[1, 2] +  182 * objective_arguments[2, 2] - 1493 / 50],\n",
    "#                      [ 42 * objective_arguments[0, 2] + 182 * objective_arguments[1, 2] +  882 * objective_arguments[2, 2] -  607 /  5],\n",
    "#                      [182 * objective_arguments[0, 2] + 882 * objective_arguments[1, 2] + 4550 * objective_arguments[2, 2] - 2876 /  5]], dtype=float).reshape((3, 3)).T\n",
    "\n",
    "\n",
    "# print(quadratic_gradient(W))\n",
    "\n",
    "\n",
    "def gradient(objective_arguments):\n",
    "    # dE/dW = 2X(XW-T) using the chain rule, but the dimensions of X and (XW-T) are not compatible, so X is transposed.\n",
    "    return 2 * T.T @ (T @ objective_arguments - P)\n",
    "\n",
    "\n",
    "# Constant speed solution:\n",
    "#      1.4693    2.1260    0.7993\n",
    "#     -0.4417   -0.5903    0.4826\n",
    "# Constant acceleration solution:\n",
    "#       5.5160   -0.8940    1.3110\n",
    "#      -3.4767    1.6747    0.0988\n",
    "#       0.4336   -0.3236    0.0548\n",
    "def gradient_descent(objective_arguments, objective_gradient, learning_rate=1e-5, max_num_iterations=1e+10,\n",
    "                     step_eps=1e-15):\n",
    "    for i in range(int(max_num_iterations)):\n",
    "        step = learning_rate * objective_gradient(objective_arguments)\n",
    "        if np.amax(np.abs(step)) < step_eps:\n",
    "            break\n",
    "        objective_arguments -= step\n",
    "    return objective_arguments\n",
    "\n",
    "\n",
    "print(\"Model Weights: \\n\", gradient_descent(W, gradient))\n",
    "print(\"Residual Error: \\n\", np.sum((P - T @ W)) ** 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
